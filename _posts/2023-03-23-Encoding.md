---
layout: post
title:  "Octetos y codificación de caracteres"
date:   2023-03-10 14:46:44 +0100
categories: códigos codificación 
author: "by rnek0"
lang: "es"
---

Este post esta en espera de traducción, se puede leer en francés en mi capsula [Gemini](gemini://gmi.lunarviews.net) en IPV6 o pasando por [Gemini portal](https://portal.mozz.us/gemini/gmi.lunarviews.net/index.gmi)

# Como comprender y saber si ando con little endian ?

Este texto esta en borrador y traducción, se puede ya leer la version alfa ya que esta bastante avanzado. Se trata de conocer poco a poco diferentes conceptos sobre numeración y codificación antes de ir a por cosas en assembler (asm) o reverse.

Para probar que estoy en little endian me serviré de Unicode, os advierto que si no sabéis de que hablo, voy a intentar de aliviaros la cosa para que os sentáis mas cómodo con los ordenadores.

Venga, para empezar este post nos serviremos del valor de la eñe , que es esta en Unicode __\u00f1__

Un ejemplo con este comando en bash que produce la salida Hackerñol

```bash
echo -e "Hacker\u00f1ol\n"
Hackerñol

```

Todos conocemos (o casi) el comando echo, el argumento -e permite de evitar de interpretar los caracteres d'escape ( los que empiezan por una barra invertida __\\__ ), que es el terminal ? Es una manera de dar ordenes a tu ordenador, (antiguamente no había interfaces gráficas con ventanas y ratón); usando comandos puedes dialogar con el ordenador, el comando **echo** hace una salida por pantalla de texto. Se puede pues insertar en la cadena de texto este valor **\u00f1** que es el correspondiente en el punto de código Unicode de la letra __ñ__

## Que es Unicode ?

Un ordenador solo comprende una serie de cifras, no conoce ningún idioma. Ni en ingles, ni el francés ni tampoco el chino.

La únicas cifras que el ordenador comprende son el uno y el zero ( 0 ou 1 ). De hecho interiormente tampoco conoce esas cifras, digamos mejor que sabe hacer la diferencia entre dos estados en sus circuitos. La corriente eléctrica pasa o no pasa. Por comodidad se escogió el decir que si la corriente pasa, sera el 1, en cambio si la corriente no pasa se dirá que es el cero. El caso contrario también es cierto, se podría haber dicho lo contrario, es una convención. Lo cierto es que solo disponemos de dos estados mas tarde veremos que un bit (la unidad minima de información) tiene dos estados. Se quiso pues que algo que se parecía a una bombilla de luz pudiera servir a hacer cálculos, o esta en un estado encendió o en un estado apagado. Y un dia un hombre genial invento (o descubrió) los números binarios... 

Si pero eso no responde a la pregunta, Unicode és una representación codificada de los caracteres que pueden ser utilizados por todos los idiomas, lo vamos a ver un poco mas tarde (su interés y utilización)

## Savez vous compter ?

Nous les humains on dispose de deux mains, et oui !, avec en general 5 doigts chacune. Donc du coup on a peût être choisi la base 10 (nombre de doigts) comme base de calcul. Du coup notre façon classique de faire les comptes se base sur un système décimal, on se sert seulement de 10 chiffres pour representer un nombre. Nôtre ami l'ordinateur n'a pas de doigts et ne comprends pas autre chose que le langage de l'ampoule (allumée ou éteinte). Il a donc une base de calcul de deux états (soit 1, soit 0) cette base sera donc apellée binaire.  

Donc, nous on se sert du __décimal__ et l'ordinateur du __binaire__.

Allez, on commence a compter : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 ( ceci sont nos chiffres ), mais pour aller plus loin comme on n'a que ces 10 chiffres et bien on va ajouter 1 a gauche et a droite on reviens a nôtre chiffre de départ. Cela donne 10, nous sommes habitués a le faire et on trouve ça normal. Si on arrive a 19 on rajoute 1 a gauche et on continue donc 20, on a les chiffres pour ça mais qu'est ce qu'on fait arrivés a 99 ? La même chose que quand on arrive a 9, on ajoute 1 a gauche et on remets les 9 a zero. A chaque fois on ajoute une colonne et donc en multiplant le numero de la colonne par la base (10), puis l'addition de toutes les colonnes nous donne le nombre que l'on veut exprimer.  La première colonne a une valeur de 0 par la base car on n'a pas encore additionné aucune colonne. Donc pour prendre un exemple simple (avec le nombre 150) :  

```
150 = 1 x 10² + 5 x 10¹ + 0 x 10⁰
```

Pour les curieux : comment on écrit ceci avec un peu d'unicode ?

```
echo -e "1 x 10\u00b2 + 5 x 10\u00b9 + 0 x 10\u2070"
```

Maintenant au tour de notre ami l'ordi. 0, 1 ( ceci sont ses chiffres ), alors qu'est ce qu'il fait quand il arrive a 1 ? Et oui, comme nous il ajoute une colonne a gauche avec un 1 et il remet la première colonne a 0 ! L'ordre des colonnes nous donne le numéro a mettre en puissance de sa base (deux), puis on additionne les resultats de chaque colonne.  

Exemple (en base 2) 1101

1 + 2³ + 1 x 2² + 0 x 2¹ + 1 x 2⁰  
= 8 + 4 + 0 + 1  
= 13  

Encore un peu d'unicode ;)

```
echo -e "1 + 2\u00b3 + 1 x 2\u00b2 + 0 x 2\u00b9 + 1 x 2\u2070"
```

On va comparer pour voir ( a gauche c'est nous, a droite l'ordinateur ) :  

0  -> 0  
1  -> 1  
2  -> 10  
3  -> 11  
4  -> 100  
5  -> 101  
6  -> 110  
7  -> 111  
8  -> 1000  
9  -> 1001  

10 -> 1010  
11 -> 1011  
12 -> 1100  
13 -> 1101  
14 -> 1110  
15 -> 1111  
... etc ...  

Ok, c'est cool on a compris __comment compte un ordinateur \o/__

## Savez vous écrire ?

Nous les humains nous avons des lettres en plus des nombres, du coup on peut écrire des choses comme ce texte que vous lisez. Nôtre ami l'ordinateur n'a pas de lettres, alors comment fait-il ?  
Et bien pour cela on a inventé des conventions qui font que certains nombres dans l'ordinateur vont toujours correspondre a certaines lettres ou caractères. On a appellé cela  le codage de caractères. Les premiers n'étaient pas compatibles les uns avec les autres mais finalement on trouva un standard : ce fut le code [ASCII](https://fr.wikipedia.org/wiki/American_Standard_Code_for_Information_Interchange).  

Le code ASCII est l'ancêtre du fameux [__Unicode__](https://es.wikipedia.org/wiki/Unicode) dont je parle au debut de ce texte au sujet de la lettre ñ.

Pour faire simple on va dire que la memoire de l'ordinateur est composée de rangées qui ont chacune 8 emplacements. 8 ampoules par rangée, donc 8 chiffres binaires qui peuvent être soit 0 ou 1 et que l'on nomme des bits. On a utilisé 7 bits pour coder les caractères, et donc par exemple le code binaire __0100 0001__ representera toujours la lettre A en code ASCII.  

Ce code, bien qu'ayant été un standard n'était pas assez évolué pour tenir compte de tous les caractères de tous les languages de la planète terre. Il y a eu des codes supplementaires pour complèter celui-ci, par exemple pour avoir les lettres accentuées ou autres caractères qui n'avaient pas été prevus. Mais du coup on commençait a avoir des incompatibilités de codage, entre ces codes.

De nos jours, ce codage a évolué, le standard actuel est Unicode. Unicode englobe ASCII, ce qui veut dire que les premiers codes ASCII sont respectés, mais comme Unicode possède plus de codes ( car plus de caractères a chiffrer ) il prends plus de place dans la memoire de l'ordinateur. Pour donner un exemple si on prends __UTF-8__ qui est une des trois possibilités reconnues par Unicode et pratiquement généralisé sur toute la planète, les emplacements mémoire requis pour encoder les caractères vont de 1 a 4 octets (soit de 8 a 32 bit).  
Pour ceux qui sont curieux UTF-8 a été crée par Robert C. Pike y Kenneth L. Thompson. Le premier a également crée le langage Go et le deuxième est consideré comme étant un des pères de UNIX avec Mr. Dennis Ritchie qui fut le créateur du Langage C.  

## Savez vous ranger ?

Nous savons maintenant que le language des ordinateurs est le binaire, c'est le __code machine__ que le processeur va traiter lorsqu'il execute un programme informatique. C'est une suite de bits composant des instructions pour le processeur et chaque famille de processeurs a les siennes. Ces instructions sont uniques avec des fonctionalités bien definies, quand il a fini d'executer une instruction il passe a la suivante. Or pour pouvoir dire a l'ordinateur ce qu'il doit faire, nous les humains ne pouvons pas passer nôtre temps a compter et écrire des suites interminables de 0 et de 1. On doit donc trouver la façon de ranger les instructions de telle sorte qu'on puisse les reconnaitre et qu'on puisse se mettre d'accord avec l'ordinateur pour lui dire ce qu'il doit faire.

On a donc commencé par donner des noms simples a ces instructions que le processeur sait traiter. Par exemple le processeur losqu'il va rencontrer ceci : __10110000 01100001__ va __reconnaitre__ l'instruction a executer. On a donc attribué des noms significatifs pour les humains a ces types d'instructions qu'on appelle des __mnemoniques__ ainsi l'instruction précédente deviens ceci ( dans un language plus comprehensible pour nous ) :

asm  
```
movb $0x61,%ali
```

D'autre part, on voit bien que l'on separe donc les bits en groupes de 8 bit (octets), c'est déjà un premier rangement. On voit bien que ce n'est pas très facile d'interpreter cela a vue d'oeuil, on va donc utiliser une autre façon de compter pour mieux visualiser la chose. Et la, pour mieux ranger on va passer par un système numérique en base 16 au lieu du binaire qu'on a déjà vu (en base2). Comment cela est possible ? et bien on va dire que les chiffres qui definissent les 16 elements iront de 0 a 9 comme en décimal, mais comme il nous en faut d'autres et qu'ils doivent être differents des précédents on va les symboliser par des lettres allant de A à F. Cela donne donc ceci : 0, 1, 2, 3 ,4 ,5 ,6, 7, 8, 9, A, B, C, D, E, F

Comment on fait alors ? On compte de la même façon qu'avec les autres bases, de 0 a F et arrivé a F on met une deuxième colonne a gauche et on remet a zero a droite ;)  

Exemple en hexadécimal avec le nombre __2BD__ en hexadécimal :

Ici on va representer les puissances comme dans un language nommé Python, c'est à dire que (16 ** 2) équivaut a 16² , ce sera peut-être plus parlant :

Python  
```
2BD = 2 × (16 ** 2) + 11 × (16 ** 1) + 13 × (16 ** 0)
512 + 176 + 13
701
```

Donc, pour simplifier, puisque ( 255 ) en base 10 est égal a ( FF ) en base 16 et bien avec deux digits en base 16 c'est pareil que 8 bits en binaire \o/ .

Autrement dit : puisqu'un octet peut prendre 256 valeurs (soit 2^8) on peut décrire chaque octet avec deux chiffres hexadémaux.

Il vaut mieux lire FF que 11111111  !!!

On va donc ranger nos octects par paires de nombres hexadécimaux, ce sera donc plus visible. Question : avez vous remarqué ceci ( __$0x61__ ) dans la ligne plus haut ?

## Enfin on arrive a little endian

C'est trés bien tout ça, mais... Qu'est-ce que cela a avoir avec [little endian](https://en.wikipedia.org/wiki/Endianness) ?

On a parlé d'instructions un peu plus haut et du fait que chaque processeur a les siennes. Comme elles sont toutes en binaire, il doit y avoir une taille determinée pour que le processeur les identifie ( tous les processeurs, n'ont pas la même taille de traitement ) Et bien cette taille c'est ce que l'on nomme un [mot](https://en.wikipedia.org/wiki/Word_(computer_architecture)) ( word en anglais ), on parle de processeurs 8, 16, 32 ou 64 bits mais il y en a d'autres.

Il y a donc deux tendances pour savoir __dans quel sens les mots sont écrits__ pour être traités dans les differentes mémoires de l'ordinateur, petit boutisme ou gros boutisme, " In computing, endianness is the order or sequence of bytes of a word of digital data in computer memory. [Endianness](https://en.wikipedia.org/wiki/Endianness) " On va tenter de savoir dans quel sens vont les nôtres ;)  

Voici nôtre exemple : on va écrire le mot Hackerñol a l'ecran, mais on a un clavier français...

1. premier essai:

```
echo "Hackernol"
```

on a pas la bonne touche, alors on se dit que le caractère existe et qu'il doit bien être dans la table unicode, on le cherche et on tombe sur celui cité plus haut dans l'article ( \\u00f1 )

2. Deuxième essai on utilise unicode ( donc 1 a 4 octets )

```
echo -e "Hacker\u00f1ol\n"
Hackerñol
```

On obtiens un bon résultat a l'écran, mais on ne sais toujours pas si on est en little endian ou pas.

3. Troisième essai

Maintenant, on va regarder plus en profondeur avec un programme qui s'appelle __hexdump__

```
❯ printf "Hacker\u00f1ol\n" | hexdump
0000000 6148 6b63 7265 b1c3 6c6f 000a          
000000b
```

On commence a voir du code hexadécimal :) Mais qu'est ce que ce code veut dire ? Si on tentait de le convertir en Unicode ?

Allez ! on commence par les 4 premiers chiffres en hexadécimal :

```
❯ echo -e "\x61\x48"
aH
```

Tiens ! c'est bizarre le __a__ et le __H__ sont inversés !!! 

Bon mais de quoi on parle a la fin ?  
En informatique, certaines données telles que les nombres entiers peuvent être représentées sur plusieurs octets. L'ordre dans lequel ces octets sont organisés en mémoire ou dans une communication est appelé endianness. De la même manière que certains langages humains s'écrivent de gauche à droite, et d'autres s'écrivent de droite à gauche[1], il existe une alternative majeure à l'organisation des octets représentant une donnée : l'orientation big-endian et l'orientation little-endian. Ces expressions sont parfois traduites par gros-boutiste et petit-boutiste. Les expressions byte order, d'ordre des octets ou de byte sex sont également utilisées.

L'endianness ne concerne que les données structurées sur plusieurs octets, telles que les nombres entiers, ou les caractères Unicode, codées en UTF-16 ou UTF-32. Les données codées sur un seul octet, telles que les caractères ASCII ne sont pas concernées. (Definition sauvagement copiée sur wikipédia)

=> https://fr.wikipedia.org/wiki/Boutisme Boutisme.

=> https://unicodebook.readthedocs.io/unicode_encodings.html#byte-order-marks-bom  
ATTENTION ! apparemment UTF-8 serait endianless chose que je ne savais pas, je pense que les gens ont confondu avec BOM et le fait que ce n'est pas obligatoire de le mettre. Le mieux est de jetter un coup d'oeuil ici si cela vous interesse vraiement :  
=> https://datatracker.ietf.org/doc/html/rfc3629#page-4 UTF-8 

* Alors quel est le boutisme ?  
* Quelle est la taille du mot ?
* Qu'est-ce que le codage ? ASCII, Unicode, UTF-8
* Décimal, Binaire, Hexadécimal...

Je pense qu'on a appris plein de choses, peut-être relire un peu et faire tes recherches pour mieux comprendre et assimiler ;)

J'avoue que c'est un peu bruteforcer les neurones pour arriver a un resultat que l'on connais. La plupart des ordinateurs actuels fonctionnent en little endian. Mais Attention, le protocole TCP le fais en big-endian ! Bon, mais c'est prouvé et compris. Enfin j'espère. Allez, pour la forme je vous mets le tout ;)

```bash
❯ echo $'\x48\x61\x63\x6b\x65\x72\xc3\xb1\x6f\x6c'
Hackerñol
```

Cet article m'est venu à l'idée en lisant celui-ci [Understanding pointers sur Drew DeVault's blog](https://drewdevault.com/2016/05/28/Understanding-pointers.html) et en postant des trucs dans le matrix de hispagatos.

Bonne journée ;)

Amfora tip of the day :

 Space                Open bar at the bottom - type a URL, link number, search term.
                      You can also type two dots (..) to go up a directory in the URL.
                      Typing new:N will open link number N in a new tab
                      instead of the current one.

