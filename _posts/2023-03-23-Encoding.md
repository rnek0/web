---
layout: post
title:  "Octetos y codificación de caracteres"
date:   2023-03-10 14:46:44 +0100
categories: códigos codificación 
author: "by rnek0"
lang: "es"
---

Este post esta en espera de traducción, se puede leer en francés en mi capsula [Gemini](gemini://gmi.lunarviews.net) en IPV6 o pasando por [Gemini portal](https://portal.mozz.us/gemini/gmi.lunarviews.net/index.gmi)

# Como comprender y saber si ando con little endian ?

Este texto esta en borrador y traducción, se puede ya leer la version alfa ya que esta bastante avanzado. Se trata de conocer poco a poco diferentes conceptos sobre numeración y codificación antes de ir a por cosas en assembler (asm) o reverse.

Para probar que estoy en little endian me serviré de Unicode, os advierto que si no sabéis de que hablo, voy a intentar de aliviaros la cosa para que os sentáis mas cómodo con los ordenadores.

Venga, para empezar este post nos serviremos del valor de la eñe , que es esta en Unicode __\u00f1__

Un ejemplo con este comando en bash que produce la salida Hackerñol

```bash
echo -e "Hacker\u00f1ol\n"
Hackerñol

```

Todos conocemos (o casi) el comando echo, el argumento -e permite de evitar de interpretar los caracteres d'escape ( los que empiezan por una barra invertida __\\__ ), que es el terminal ? Es una manera de dar ordenes a tu ordenador, (antiguamente no había interfaces gráficas con ventanas y ratón); usando comandos puedes dialogar con el ordenador, el comando **echo** hace una salida por pantalla de texto. Se puede pues insertar en la cadena de texto este valor **\u00f1** que es el correspondiente en el punto de código Unicode de la letra __ñ__

## Que es Unicode ?

Un ordenador solo comprende una serie de cifras, no conoce ningún idioma. Ni en ingles, ni el francés ni tampoco el chino.

La únicas cifras que el ordenador comprende son el uno y el zero ( 0 ou 1 ). De hecho interiormente tampoco conoce esas cifras, digamos mejor que sabe hacer la diferencia entre dos estados en sus circuitos. La corriente eléctrica pasa o no pasa. Por comodidad se escogió el decir que si la corriente pasa, sera el 1, en cambio si la corriente no pasa se dirá que es el cero. El caso contrario también es cierto, se podría haber dicho lo contrario, es una convención. Lo cierto es que solo disponemos de dos estados mas tarde veremos que un bit (la unidad minima de información) tiene dos estados. Se quiso pues que algo que se parecía a una bombilla de luz pudiera servir a hacer cálculos, o esta en un estado encendió o en un estado apagado. Y un dia un hombre genial invento (o descubrió) los números binarios... 

Si pero eso no responde a la pregunta, Unicode és una representación codificada de los caracteres que pueden ser utilizados por todos los idiomas, lo vamos a ver un poco mas tarde (su interés y utilización)

## ¿Sabes contar?

Los humanos disponemos de dos manos, si claro, y en general con 5 dedos cada una. Por ello es posible que se escogió la base 10 (numero de dedos) comme base de calculo. Así que nuestra forma clásica de hacer las cuentas se basa en un sistema decimal, solo se usan 10 dígitos o cifras para representar un número. Nuestro amigo el ordenador no tiene dedos y no entiende nada más que el idioma de la bombilla (encendida o apagada). Tiene por tanto una base de cálculo de dos estados (ya sea 1 o 0) por lo que esta base se denominará binaria.

Entonces, nosotros usamos __decimal__ y la computadora usa __binario__.

Vamos, empecemos a contar: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 (estas son nuestras cifras), pero para ir más allá, ya que solo tenemos estos 10 dígitos, pues sumaremos 1 a la izquierda y a la derecha volvemos a nuestro dígito inicial (el 0). Esto da 10, estamos acostumbrados a hacerlo y nos parece normal. Si llegamos a 19 sumamos 1 a la izquierda y por lo tanto continuamos 20, tenemos las cifras para eso, pero ¿qué hacemos cuando lleguemos a 99 ? Lo mismo que cuando llegamos al 9, sumamos 1 a la izquierda y ponemos el 9 a cero. Cada vez que sumamos una columna y por lo tanto al multiplicar el número de la columna por la base (10), entonces la suma de todas las columnas, nos da el número que queremos expresar.La primera columna tiene un valor de 0 para la base porque aún no hemos agregado ninguna columna. Entonces, para tomar un ejemplo simple (con el número 150) :  

```bash
150 = 1 x 10² + 5 x 10¹ + 0 x 10⁰
```

Para los curiosos: ¿cómo se escribe esto de aquí arriba con un poco de Unicode y el comando echo?

```bash
echo -e "150 = 1 x 10\u00b2 + 5 x 10\u00b9 + 0 x 10\u2070"
```

Ahora le toca el turno a nuestro amigo el ordenador. 0, 1 (estas son sus cifras), entonces, ¿qué hace cuando llega al 1? Pues sí, como nosotros, agrega una columna a la izquierda con un 1 y vuelve a colocar la primera columna en 0. Seguimos la misma lógica que con la base décimal; el orden de las columnas nos da el número a poner en potencia de su base (aquí sera dos), luego sumamos los resultados de cada columna. 

Ejemplo (en base 2) 1101

1 + 2³ + 1 x 2² + 0 x 2¹ + 1 x 2⁰  
= 8 + 4 + 0 + 1  
= 13  

Un poco más Unicode ;)

```bash
echo -e "1 + 2\u00b3 + 1 x 2\u00b2 + 0 x 2\u00b9 + 1 x 2\u2070"
1 + 2³ + 1 x 2² + 0 x 2¹ + 1 x 2⁰
```

Compararemos para ver (a la izquierda somos nosotros, a la derecha la computadora):  

0  -> 0  
1  -> 1  
2  -> 10  
3  -> 11  
4  -> 100  
5  -> 101  
6  -> 110  
7  -> 111  
8  -> 1000  
9  -> 1001  

10 -> 1010  
11 -> 1011  
12 -> 1100  
13 -> 1101  
14 -> 1110  
15 -> 1111  
... etc ...  

Vale eso es genial, descubrimos __cómo cuenta una computadora \o/__

## ¿Sabes escribir?

Los humanos tenemos letras además de números, así que podemos escribir cosas como este texto que estás leyendo. Nuestro amigo el ordenador no tiene letras, entonces ¿cómo lo hace?  
Bueno, para eso inventamos convenciones que hacen que ciertos números en la computadora siempre se correspondan con ciertas letras o caracteres. Esto se ha llamado "[codificación de caracteres](https://es.wikipedia.org/wiki/Codificaci%C3%B3n_de_caracteres)". Los primeros no eran compatibles entre sí pero finalmente encontramos un estándar: este fué el código [ASCII](https://fr.wikipedia.org/wiki/American_Standard_Code_for_Information_Interchange).  

El código ASCII es el antepasado del famoso [__Unicode__](https://es.wikipedia.org/wiki/Unicode) del que hablo al principio de este texto, por ejemplo con la letra ñ.

En pocas palabras, diremos que la memoria de la computadora se compone de filas que tienen 8 ranuras cada una. 8 bombillas por fila, por lo tanto 8 dígitos binarios que pueden ser 0 o 1 y que se llaman bits. Usamos 7 bits para codificar los caracteres, y por lo tanto, por ejemplo, el código binario __0100 0001__ siempre representará la letra A en código ASCII. Y el octavo ? pues sirve para hacer verificaciones de errores de transmisión. 

Este código, a pesar de haber sido un estándar, no fue lo suficientemente evolucionado para tener en cuenta todos los caracteres de todos los idiomas del planeta tierra. Había códigos adicionales para completar el código ASCII, por ejemplo para tener letras acentuadas u otros caracteres que no se habían previsto. Pero de repente empezamos a tener incompatibilidades de codificación entre estos códigos.

Hoy en día, esta codificación ha evolucionado, el estándar actual es Unicode. Unicode incluye ASCII, lo que significa que se respetan los primeros códigos ASCII, pero como Unicode tiene más códigos (porque hay más caracteres para cifrar) ocupa más espacio en la memoria de la computadora. Por poner un ejemplo si tomamos __UTF-8__ que es una de las tres posibilidades reconocidas por Unicode y prácticamente generalizadas en todo el planeta, las posiciones de memoria requeridas para codificar los caracteres van de 1 a 4 bytes (es decir de 8 a 32 bit ).
Para los curiosos, UTF-8 fue creado por Robert C. Pike y Kenneth L. Thompson. El primero también creó el lenguaje Go y el segundo es considerado uno de los padres de UNIX con el Sr. Dennis Ritchie quien fue el creador del lenguaje C. 

## ¿Sabes cómo ordenar?

Nous savons maintenant que le language des ordinateurs est le binaire, c'est le __code machine__ que le processeur va traiter lorsqu'il execute un programme informatique. C'est une suite de bits composant des instructions pour le processeur et chaque famille de processeurs a les siennes. Ces instructions sont uniques avec des fonctionalités bien definies, quand il a fini d'executer une instruction il passe a la suivante. Or pour pouvoir dire a l'ordinateur ce qu'il doit faire, nous les humains ne pouvons pas passer nôtre temps a compter et écrire des suites interminables de 0 et de 1. On doit donc trouver la façon de ranger les instructions de telle sorte qu'on puisse les reconnaitre et qu'on puisse se mettre d'accord avec l'ordinateur pour lui dire ce qu'il doit faire.

On a donc commencé par donner des noms simples a ces instructions que le processeur sait traiter. Par exemple le processeur losqu'il va rencontrer ceci : __10110000 01100001__ va __reconnaitre__ l'instruction a executer. On a donc attribué des noms significatifs pour les humains a ces types d'instructions qu'on appelle des __mnemoniques__ ainsi l'instruction précédente deviens ceci ( dans un language plus comprehensible pour nous ) :

asm  
```
movb $0x61,%ali
```

D'autre part, on voit bien que l'on separe donc les bits en groupes de 8 bit (octets), c'est déjà un premier rangement. On voit bien que ce n'est pas très facile d'interpreter cela a vue d'oeuil, on va donc utiliser une autre façon de compter pour mieux visualiser la chose. Et la, pour mieux ranger on va passer par un système numérique en base 16 au lieu du binaire qu'on a déjà vu (en base2). Comment cela est possible ? et bien on va dire que les chiffres qui definissent les 16 elements iront de 0 a 9 comme en décimal, mais comme il nous en faut d'autres et qu'ils doivent être differents des précédents on va les symboliser par des lettres allant de A à F. Cela donne donc ceci : 0, 1, 2, 3 ,4 ,5 ,6, 7, 8, 9, A, B, C, D, E, F

Comment on fait alors ? On compte de la même façon qu'avec les autres bases, de 0 a F et arrivé a F on met une deuxième colonne a gauche et on remet a zero a droite ;)  

Exemple en hexadécimal avec le nombre __2BD__ en hexadécimal :

Ici on va representer les puissances comme dans un language nommé Python, c'est à dire que (16 ** 2) équivaut a 16² , ce sera peut-être plus parlant :

Python  
```
2BD = 2 × (16 ** 2) + 11 × (16 ** 1) + 13 × (16 ** 0)
512 + 176 + 13
701
```

Donc, pour simplifier, puisque ( 255 ) en base 10 est égal a ( FF ) en base 16 et bien avec deux digits en base 16 c'est pareil que 8 bits en binaire \o/ .

Autrement dit : puisqu'un octet peut prendre 256 valeurs (soit 2^8) on peut décrire chaque octet avec deux chiffres hexadémaux.

Il vaut mieux lire FF que 11111111  !!!

On va donc ranger nos octects par paires de nombres hexadécimaux, ce sera donc plus visible. Question : avez vous remarqué ceci ( __$0x61__ ) dans la ligne plus haut ?

## Enfin on arrive a little endian

C'est trés bien tout ça, mais... Qu'est-ce que cela a avoir avec [little endian](https://en.wikipedia.org/wiki/Endianness) ?

On a parlé d'instructions un peu plus haut et du fait que chaque processeur a les siennes. Comme elles sont toutes en binaire, il doit y avoir une taille determinée pour que le processeur les identifie ( tous les processeurs, n'ont pas la même taille de traitement ) Et bien cette taille c'est ce que l'on nomme un [mot](https://en.wikipedia.org/wiki/Word_(computer_architecture)) ( word en anglais ), on parle de processeurs 8, 16, 32 ou 64 bits mais il y en a d'autres.

Il y a donc deux tendances pour savoir __dans quel sens les mots sont écrits__ pour être traités dans les differentes mémoires de l'ordinateur, petit boutisme ou gros boutisme, " In computing, endianness is the order or sequence of bytes of a word of digital data in computer memory. [Endianness](https://en.wikipedia.org/wiki/Endianness) " On va tenter de savoir dans quel sens vont les nôtres ;)  

Voici nôtre exemple : on va écrire le mot Hackerñol a l'ecran, mais on a un clavier français...

1. premier essai:

```
echo "Hackernol"
```

on a pas la bonne touche, alors on se dit que le caractère existe et qu'il doit bien être dans la table unicode, on le cherche et on tombe sur celui cité plus haut dans l'article ( \\u00f1 )

2. Deuxième essai on utilise unicode ( donc 1 a 4 octets )

```
echo -e "Hacker\u00f1ol\n"
Hackerñol
```

On obtiens un bon résultat a l'écran, mais on ne sais toujours pas si on est en little endian ou pas.

3. Troisième essai

Maintenant, on va regarder plus en profondeur avec un programme qui s'appelle __hexdump__

```
❯ printf "Hacker\u00f1ol\n" | hexdump
0000000 6148 6b63 7265 b1c3 6c6f 000a          
000000b
```

On commence a voir du code hexadécimal :) Mais qu'est ce que ce code veut dire ? Si on tentait de le convertir en Unicode ?

Allez ! on commence par les 4 premiers chiffres en hexadécimal :

```
❯ echo -e "\x61\x48"
aH
```

Tiens ! c'est bizarre le __a__ et le __H__ sont inversés !!! 

Bon mais de quoi on parle a la fin ?  
En informatique, certaines données telles que les nombres entiers peuvent être représentées sur plusieurs octets. L'ordre dans lequel ces octets sont organisés en mémoire ou dans une communication est appelé endianness. De la même manière que certains langages humains s'écrivent de gauche à droite, et d'autres s'écrivent de droite à gauche[1], il existe une alternative majeure à l'organisation des octets représentant une donnée : l'orientation big-endian et l'orientation little-endian. Ces expressions sont parfois traduites par gros-boutiste et petit-boutiste. Les expressions byte order, d'ordre des octets ou de byte sex sont également utilisées.

L'endianness ne concerne que les données structurées sur plusieurs octets, telles que les nombres entiers, ou les caractères Unicode, codées en UTF-16 ou UTF-32. Les données codées sur un seul octet, telles que les caractères ASCII ne sont pas concernées. (Definition sauvagement copiée sur wikipédia)

=> https://fr.wikipedia.org/wiki/Boutisme Boutisme.

=> https://unicodebook.readthedocs.io/unicode_encodings.html#byte-order-marks-bom  
ATTENTION ! apparemment UTF-8 serait endianless chose que je ne savais pas, je pense que les gens ont confondu avec BOM et le fait que ce n'est pas obligatoire de le mettre. Le mieux est de jetter un coup d'oeuil ici si cela vous interesse vraiement :  
=> https://datatracker.ietf.org/doc/html/rfc3629#page-4 UTF-8 

* Alors quel est le boutisme ?  
* Quelle est la taille du mot ?
* Qu'est-ce que le codage ? ASCII, Unicode, UTF-8
* Décimal, Binaire, Hexadécimal...

Je pense qu'on a appris plein de choses, peut-être relire un peu et faire tes recherches pour mieux comprendre et assimiler ;)

J'avoue que c'est un peu bruteforcer les neurones pour arriver a un resultat que l'on connais. La plupart des ordinateurs actuels fonctionnent en little endian. Mais Attention, le protocole TCP le fais en big-endian ! Bon, mais c'est prouvé et compris. Enfin j'espère. Allez, pour la forme je vous mets le tout ;)

```bash
❯ echo $'\x48\x61\x63\x6b\x65\x72\xc3\xb1\x6f\x6c'
Hackerñol
```

Cet article m'est venu à l'idée en lisant celui-ci [Understanding pointers sur Drew DeVault's blog](https://drewdevault.com/2016/05/28/Understanding-pointers.html) et en postant des trucs dans le matrix de hispagatos.

Bonne journée ;)

Amfora tip of the day :

 Space                Open bar at the bottom - type a URL, link number, search term.
                      You can also type two dots (..) to go up a directory in the URL.
                      Typing new:N will open link number N in a new tab
                      instead of the current one.

